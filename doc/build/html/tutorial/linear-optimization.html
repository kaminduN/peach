

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Linear Optimization of a Single Variable &mdash; Peach v0.3.1 documentation</title>
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.3.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Peach v0.3.1 documentation" href="../index.html" />
    <link rel="up" title="Tutorials" href="tutorial.html" />
    <link rel="next" title="Linear Optimization of a Single Variable with Derivative Methods" href="derivative-optimization.html" />
    <link rel="prev" title="Fuzzy C-Means" href="fuzzy-c-means.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="derivative-optimization.html" title="Linear Optimization of a Single Variable with Derivative Methods"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="fuzzy-c-means.html" title="Fuzzy C-Means"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" accesskey="U">Tutorials</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="linear-optimization-of-a-single-variable">
<h1>Linear Optimization of a Single Variable<a class="headerlink" href="#linear-optimization-of-a-single-variable" title="Permalink to this headline">¶</a></h1>
<p>The optimization problem for a function of a single variable can be put as the
following: finding the <img class="math" src="../_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> in a function <img class="math" src="../_images/math/c96dd6ec1dc4ad7520fbdc78fcdbec9edd068d0c.png" alt="f(x)"/> that wields the
function&#8217;s smallest value. Mathematically, it is usually put as</p>
<div class="math">
<p><img src="../_images/math/d315d0d928729ff2e9add3d2fb2ae6c02f69b58b.png" alt="x^*  = \min f(x)" /></p>
</div><p>There are numerous different methods to find the optimal value of <img class="math" src="../_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/>. In
general, these methods consist in getting a first estimate <img class="math" src="../_images/math/17f1249ad95b7682b8316ad21de8ce4ee9fdcf93.png" alt="x_0"/> of the
location of the minimum and adding to it or subtracting from it an update step.
The algorithms diverge in how the update step is computed. Here is a brief
description of some of the methods implemented in Peach:</p>
<blockquote>
<dl class="docutils">
<dt>Direct</dt>
<dd>The update step is a fixed value. If the updated estimate results in the
growth of the value of the function, it means that the estimate is getting
farther from the minimum. Thus, the sign of the step is inverted, and its
magnitude is halved. This method, in general, is not very efficient, but
can be used in situations where other methods won&#8217;t perform well (for
example, if the function has discontinuities).</dd>
<dt>Interpolation</dt>
<dd>In the interpolation search, the estimate is given in the form of three
points, <img class="math" src="../_images/math/3c1b2b36f9a54685e0c9fa4874c32257d0361f01.png" alt="x_l"/>, <img class="math" src="../_images/math/fcaeea91c5bea53f0f7fb9dc6300816e6320b58f.png" alt="x_m"/> and <img class="math" src="../_images/math/bd9e219079daf3f718b08890cec41d33b0906460.png" alt="x_h"/>, such that
<img class="math" src="../_images/math/6f81cea12bb3ba6fa2490bba40fb1662e9b67e2e.png" alt="x_l &lt; x_m &lt; x_h"/>. A parabolic function passing through these points
is then computed, because in a parabolic function, it is very easy to
compute the location of the minimum. If the minimum of the parabolic
approximation is not good enough, a new set of estimates is then computed
and fed to the algorithm.</dd>
<dt>Golden Rule</dt>
<dd>In the golden rule search, the estimate is given in the form of the lower
and upper limits of an interval where the minimum is located. This interval
is partitioned in three sub-intervals using the golden ratio as the
proportion of its sizes, and a new estimate is computed by finding in which
sub-interval it is.</dd>
<dt>Fibonacci</dt>
<dd>The Fibonacci search is identical to the golden rule search, except that the
golden ratio is not explicitly given. Instead, it is approximated by the
ratio of numbers in the Fibonacci sequence. This, in general, gives worst
results than the golden rule search, but it might be needed in environments
where floating point numbers are not available.</dd>
</dl>
</blockquote>
<p>The efficiency of optimization methods are evaluated using benchmark functions.
There are lots of functions of this type, but, in general, these functions have
local minima and are difficult to minimize. One such function is the Rosenbrock
function, which one-dimensional version is given below:</p>
<div class="math">
<p><img src="../_images/math/d7d8a03809ff59371a1e38ec0ee9d5d4ee984aef.png" alt="f(x) = (1 - x)^2 + (1 - x^2)^2" /></p>
</div><p>This function, actually, is a simplified version &#8211; the actual Rosenbrock
function adds some multiplying constants that make the slopes even smaller,
making it very difficult to find the minimum. The minimum can be found
analitically (by derivatives) to be <img class="math" src="../_images/math/34310f2f36ed6d724838edb08788ee62acb33386.png" alt="x = 1"/>. We will proceed with the
steps to optimize this function with the direct search optimizer.</p>
<p>As always, we first import <tt class="docutils literal"><span class="pre">numpy</span></tt> for arrays and <tt class="docutils literal"><span class="pre">peach</span></tt> for the library.
Actually, <tt class="docutils literal"><span class="pre">peach</span></tt> also the <tt class="docutils literal"><span class="pre">numpy</span></tt> module, but we want it in a separate
namespace:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">peach</span> <span class="kn">as</span> <span class="nn">p</span>
</pre></div>
</div>
<p>Let&#8217;s define the function to be optimized. By using Peach, you can pass to an
optimizer any function that receives a floating point number and returns a
number. The function will be called by the optimizer, so you must ensure that it
can deal with the parameter and return an appropriate result:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>Now we will create the optimizer. Creating an optimizer is very easy: you
instantiate an optmizer by passing to it the function and the first estimate.
Some optimizers have additional parameters, please, consult the documentation
for more information on that:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">linear</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Direct1D</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
<p>The search interval can be restricted by adding a parameter given in the form of
a tuple, where the first value is the lower limit of the search interval, and
the second value is its upper limit. This parameter, however, is not needed, and
is restricted to the <tt class="docutils literal"><span class="pre">Direct1D</span></tt> (among the linear search methods), since
intervals are inherent to the other methods.</p>
<p>Every optimizer has two additional parameters that can be specified at
instantiation time. The <tt class="docutils literal"><span class="pre">emax</span></tt> parameter estipulates what will be the maximum
error allowed. The <tt class="docutils literal"><span class="pre">imax</span></tt> parameter estipulates the maximum number of
iterations the algorithm will perform. Their default values are, respectivelly,
<img class="math" src="../_images/math/726ccb1c5b1dd30ff6544e6a48732f1caa842856.png" alt="10^{-8}"/> and 1000. If we wanted different values, say, an error of 0.001
or 500 iterations, restricted on the interval (0, 2), we could create the
optimizer by issuing the following command:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">linear</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Direct1D</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">),</span> <span class="n">emax</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">imax</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
<p>Also, every optimizer has three interfaces. The <tt class="docutils literal"><span class="pre">step()</span></tt> method is called
without any parameters and computes the next estimate. It returns a tuple
<tt class="docutils literal"><span class="pre">(x,</span> <span class="pre">e)</span></tt>, where <tt class="docutils literal"><span class="pre">x</span></tt> is the new estimate and <tt class="docutils literal"><span class="pre">e</span></tt> is an estimate of the
error. The <tt class="docutils literal"><span class="pre">restart()</span></tt> method can be used to reset the algorithm, allowing to
change the estimate or another parameters. The <tt class="docutils literal"><span class="pre">__call__()</span></tt> method is also
called without any parameters, and computes the best estimate until a given
precision or a maximum number of iterations of the algorithm are achieved. The
<tt class="docutils literal"><span class="pre">x</span></tt> property holds the last estimate of the minimum, and can be read or
written at any time. However, we suggest that, if you need to write to the
estimate, that the optimizer is reset using the <tt class="docutils literal"><span class="pre">restart()</span></tt> method.</p>
<p>We will execute the algorithm step by step. We can do this to keep track of the
estimates to plot a graphic. We do this using the commands:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">xl</span> <span class="o">=</span> <span class="p">[</span> <span class="p">]</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="n">linear</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">xl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">xl</span></tt> variable will hold, in sequence, the estimates. We can plot it to see
the convergence trace. The figure below is a representation of the execution of
this and the other methods in the optimization of the same function, from the
same first estimate. We can see that, in fact, the direct search is very
inefficient. The other optimizers where created in pretty much the same way the
direct optimizer, and the estimates also stored in a list.</p>
<div align="center" class="align-center"><img alt="../_images/linear-optimization.png" class="align-center" src="../_images/linear-optimization.png" /></div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="fuzzy-c-means.html"
                        title="previous chapter">Fuzzy C-Means</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="derivative-optimization.html"
                        title="next chapter">Linear Optimization of a Single Variable with Derivative Methods</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/tutorial/linear-optimization.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="derivative-optimization.html" title="Linear Optimization of a Single Variable with Derivative Methods"
             >next</a> |</li>
        <li class="right" >
          <a href="fuzzy-c-means.html" title="Fuzzy C-Means"
             >previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" >Tutorials</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2009, José Alexandre Nalon.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.5.
    </div>
  </body>
</html>