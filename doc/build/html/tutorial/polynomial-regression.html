<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Polynomial Regression &mdash; Peach v0.3.1 documentation</title>
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.3.1',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Peach v0.3.1 documentation" href="../index.html" />
    <link rel="up" title="Tutorials" href="tutorial.html" />
    <link rel="next" title="Basic Self-Organizing Maps" href="basic-som.html" />
    <link rel="prev" title="Linear Prediction of a Number Sequence" href="linear-prediction.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="basic-som.html" title="Basic Self-Organizing Maps"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="linear-prediction.html" title="Linear Prediction of a Number Sequence"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" accesskey="U">Tutorials</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="polynomial-regression">
<h1>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h1>
<p>The learning algorithm of neural networks are based, mainly, in the mean squared
error of the output, considering the desired output of the network. The same
criterium is used for a lot of other types of approximation. The most used, and
one of the first, is the linear regression, created by the German sematician
Carl Friederich Gauss to track the Ceres asteroid with great success. In this,
the relation between two variables is approximated by a straight line. The
theory for the linear regression can be easily expanded to approximate functions
by polynomials, but in general, the equations are not simple.</p>
<p>If this is the case, a function can be approximated by</p>
<div class="math">
<p><img src="../_images/math/5166ddc0411c2b4cb2ca713180f190571aa560da.png" alt="f(x) = a_0 + a_1 x + a_2 x^2 + \ldots + a_N x^N" />
</div></p><p>It is easy to see that it can be mapped in a neuron, since it is the linear
combination of the powers of the variable, plus a linear coefficient. A single
biased neuron with <em>N+1</em> inputs, one output and linear activation can do
this job. The advantage of using a neuron, in this case, is that it is easier to
deal with the neuron than with the complicated matrix equations that the method
generates.</p>
<p>As always, we import <tt class="docutils literal"><span class="pre">numpy</span></tt> for arrays and <tt class="docutils literal"><span class="pre">peach</span></tt> for the library. Also,
we import <tt class="docutils literal"><span class="pre">random</span></tt> to generate random numbers:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">peach</span> <span class="kn">as</span> <span class="nn">p</span>
</pre></div>
</div>
<p>We create here the neural network. To make the polynomial regression, instead of
supplying the neuron with the value of the independent variable, we supply its
integer powers. The number of inputs will be, thus, the order of the polynomial
used for approximation. With this approach, our neural network will be very
simple: a single neuron with <em>N+1</em> inputs, one output, and linear activation.
The learning algorithm will be the LMS algorithm. Note that the <tt class="docutils literal"><span class="pre">bias</span></tt>
parameter is made <tt class="xref docutils literal"><span class="pre">True</span></tt>. The neural network will handle the bias
automatically:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">N</span> <span class="o">=</span> <span class="mf">10</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">FeedForward</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mf">1</span><span class="p">),</span> <span class="n">phi</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">lrule</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">LMS</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>We will map a period of a sinus. It is not expected that the coefficients found
here will be the same of the Taylor Series, since the optimization criterium is
diferent. We make some definitions to help us during the convergence:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">error</span> <span class="o">=</span> <span class="mf">1</span>
<span class="n">i</span> <span class="o">=</span> <span class="mf">0</span>
<span class="n">powers</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>  <span class="c"># This vector will be used to calculate the powers</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">powers</span></tt> vector is a vector of integers from 0 to <tt class="docutils literal"><span class="pre">N-1</span></tt>. We will use it
and the <tt class="docutils literal"><span class="pre">numpy</span></tt> constructs to calculate very efficiently the powers of the
independent variable. We will iterate the algorithm some times to obtain the
coefficients. Notice that we could generate a training set and feed it to the
network, without handling ourselves the loop. But with this we can track the
error (not shown here, but easy to add):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mf">2000</span><span class="p">:</span>

    <span class="c"># Here, we generate one value in the interval e calculate the desired</span>
    <span class="c"># response. We raise ``x`` to ``powers`` to generate the inputs. It is easy</span>
    <span class="c"># to see that the polynomial regression is a linear combination of the</span>
    <span class="c"># powers of a variable.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
    <span class="n">xo</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="n">powers</span>

    <span class="c"># We feed the network, calculate the error and teach the network</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">xo</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">1</span>
</pre></div>
</div>
<p>We can obtain the coefficients of the regression by inspecting the <tt class="docutils literal"><span class="pre">weights</span></tt>
attribute of the first (and only) layer of the network:</p>
<div class="highlight-python"><pre>print "Coefficients: "
for i in range(N):
    print "%d -&gt; %10.7f" % (i, nn[0].weights[0][i])

Coefficients:
0 -&gt;  0.0002545
1 -&gt;  3.0734091
2 -&gt;  0.0481146
3 -&gt; -4.0374620
4 -&gt; -0.1084573
5 -&gt; -1.1921023
6 -&gt; -0.4982046
7 -&gt; -0.8494203
8 -&gt; -0.2357512
9 -&gt;  0.9767743</pre>
</div>
<p>If we use the <tt class="docutils literal"><span class="pre">matplotlib</span></tt> module, we can plot the resulting mapping and the
coefficients, as shown in the figure. Notice that, as should be expected, the
coefficients of even powers are approximatelly zero. This happens because the
sinus is an odd function:</p>
<div align="center" class="align-center"><img alt="../_images/polynomial-regression.png" class="align-center" src="../_images/polynomial-regression.png" /></div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <h4>Previous topic</h4>
            <p class="topless"><a href="linear-prediction.html"
                                  title="previous chapter">Linear Prediction of a Number Sequence</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="basic-som.html"
                                  title="next chapter">Basic Self-Organizing Maps</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/tutorial/polynomial-regression.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="basic-som.html" title="Basic Self-Organizing Maps"
             >next</a> |</li>
        <li class="right" >
          <a href="linear-prediction.html" title="Linear Prediction of a Number Sequence"
             >previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" >Tutorials</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2009, José Alexandre Nalon.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.6.2.
    </div>
  </body>
</html>