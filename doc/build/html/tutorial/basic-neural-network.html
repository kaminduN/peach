<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Basic Neural Network Manipulation &mdash; Peach v0.3.1 documentation</title>
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.3.1',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Peach v0.3.1 documentation" href="../index.html" />
    <link rel="up" title="Tutorials" href="tutorial.html" />
    <link rel="next" title="Using Custom Activation Functions" href="custom-activation.html" />
    <link rel="prev" title="Tutorials" href="tutorial.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="custom-activation.html" title="Using Custom Activation Functions"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tutorial.html" title="Tutorials"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" accesskey="U">Tutorials</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="basic-neural-network-manipulation">
<h1>Basic Neural Network Manipulation<a class="headerlink" href="#basic-neural-network-manipulation" title="Permalink to this headline">¶</a></h1>
<p>The aim of this tutorial is to show how to deal with simple neural networks. We
will create a simple multi-layer perceptron (MLP), set its synaptic weights and
show the network an example. To understand this tutorial, you should have some
knowledge of how neural networks, in special MLPs. Please, consult a good text
book on the subject.</p>
<p>We will create the neural network in the figure below. As we can see, it is a
neural network with two inputs, one hidden layer with two neurons and one output
layer with one layer. We will use a sigmoidal function as its activation
function and backpropagation as its learning algorithm. These are, in general,
the default choices.</p>
<div align="center" class="align-center"><img alt="../_images/basic-neural-network.png" class="align-center" src="../_images/basic-neural-network.png" /></div>
<p>We will assume that we are using the Python command line to see what we are
doing. So, the first thing we need to do is to import the Peach library. We do
this with the command:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">peach</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
<p>Since we are using the command line, there is no problem in using this, but
remember that, usually, it is recommended that a module is not imported in the
main namespace.</p>
<p>The network we are trying to build has 2 input neurons, 2 hidden neurons and 1
input neuron. We create a new MLP by instantiating the <tt class="docutils literal"><span class="pre">FeedForward</span></tt> class, as
below:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">((</span><span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">),</span> <span class="n">Sigmoid</span><span class="p">,</span> <span class="n">BackPropagation</span><span class="p">)</span>
</pre></div>
</div>
<p>In this command line, <tt class="docutils literal"><span class="pre">(2,</span> <span class="pre">2,</span> <span class="pre">1)</span></tt> are the dimensions of each layer, in the
sequence above. The synaptic weights are randomly created and stored in a
NumPy array. You can create as many layers as you want, by passing a tuple with
the number of neurons in each layer &#8211; just remember that the first number is
the input layer, and the last number is the output layer. The Peach module takes
care of the dimension coherence.</p>
<p>We indicate the <tt class="docutils literal"><span class="pre">Sigmoid</span></tt> activation function and the <tt class="docutils literal"><span class="pre">BackPropagation</span></tt>
learning method. There are other <em>activation functions</em> and <em>learning
rules</em> available. One interesting thing is that these parameters are, actually,
classes that are internally instantiated to work with the neural network. But,
by instantiating them yourself, you can configure their behavior.</p>
<p>The complete list of parameters in the class instantiation is:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">FeedForward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">lrule</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>In this, <tt class="docutils literal"><span class="pre">layers</span></tt> is a tuple of numbers that indicate how many neurons in each
layer, where the first is the number of neurons in the input layer, the last is
the number of neurons in the output layer and the others are the number of
neurons in the hidden layers. <tt class="docutils literal"><span class="pre">phi</span></tt> is the activation function, it defaults to
the <tt class="docutils literal"><span class="pre">Linear</span></tt> function, that is, the identity function. <tt class="docutils literal"><span class="pre">lrule</span></tt> is the
learning algorithm, it defaults to the standard <tt class="docutils literal"><span class="pre">BackPropagation</span></tt>. <tt class="docutils literal"><span class="pre">bias</span></tt> is
a boolean value that indicates, if <tt class="xref docutils literal"><span class="pre">True</span></tt>, that the neurons in the network are
biased. It defaults to <tt class="xref docutils literal"><span class="pre">False</span></tt>, that is, non-biased neurons.</p>
<p>There are a number of properties that we can consult to inspect the neural
network. Some of these are given below:</p>
<blockquote>
<dl class="docutils">
<dt><tt class="docutils literal"><span class="pre">nlayers</span></tt></dt>
<dd>The number of layers in the network.</dd>
<dt><tt class="docutils literal"><span class="pre">bias</span></tt></dt>
<dd>A tuple containing the bias of each layer.</dd>
<dt><tt class="docutils literal"><span class="pre">y</span></tt></dt>
<dd>This property is the activation of the network. It corresponds to an array
with the outputs of every neuron in the last layer. This property can only
be used, however, after the network is fed some input vector.</dd>
<dt><tt class="docutils literal"><span class="pre">phi</span></tt></dt>
<dd>Activation functions for every layer in the network.</dd>
<dt><tt class="docutils literal"><span class="pre">[n]</span></tt></dt>
<dd>The <tt class="docutils literal"><span class="pre">[]</span></tt> operator can be used to access a specific <tt class="docutils literal"><span class="pre">Layer</span></tt> of the
network. There are some properties of the layers that can be very useful.</dd>
</dl>
</blockquote>
<p>A <tt class="docutils literal"><span class="pre">Layer</span></tt> is an object that represents exactly that: a layer of neurons. It
has some properties that are very useful, some of them are listed below:</p>
<blockquote>
<dl class="docutils">
<dt><tt class="docutils literal"><span class="pre">size</span></tt></dt>
<dd>The number of neurons on the layer.</dd>
<dt><tt class="docutils literal"><span class="pre">inputs</span></tt></dt>
<dd>The number of inputs in each neuron.</dd>
<dt><tt class="docutils literal"><span class="pre">shape</span></tt></dt>
<dd>A tuple in the form <tt class="docutils literal"><span class="pre">(size,</span> <span class="pre">inputs)</span></tt> with the two information above.</dd>
<dt><tt class="docutils literal"><span class="pre">weights</span></tt></dt>
<dd>A <tt class="docutils literal"><span class="pre">numpy</span></tt> array containing the synaptic weights of the neurons on the
layer. Each line of this array is the weight vector of the corresponding
neuron.</dd>
<dt><tt class="docutils literal"><span class="pre">phi</span></tt></dt>
<dd>The activation function appliedd to every neuron of the layer.</dd>
<dt><tt class="docutils literal"><span class="pre">v</span></tt></dt>
<dd>A vector containing the activation potential of the neurons of the layer.
This property is only available and can only be used after the layer was fed
an input, and will give the activation potential to the last input.</dd>
<dt><tt class="docutils literal"><span class="pre">y</span></tt></dt>
<dd>The output vector of the neurons of the layer. This property is only
available and can only be used after the layer was fed an input, and
will give the output to the last input.</dd>
</dl>
</blockquote>
<p>When the neural network is created, a randomized array of synaptic weights is
created for every layer. We can inspect and modify those using the <tt class="docutils literal"><span class="pre">weights</span></tt>
property of a <tt class="docutils literal"><span class="pre">Layer</span></tt>. The synaptic weights are <tt class="docutils literal"><span class="pre">numpy</span></tt> arrays of floating
point numbers. Let&#8217;s give our network an initial condition:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="p">[</span><span class="mf">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">array</span><span class="p">([[</span>  <span class="mf">0.5</span><span class="p">,</span>  <span class="mf">0.5</span> <span class="p">],</span>
<span class="go">                           [ -0.5, -0.5 ]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="p">[</span><span class="mf">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span> <span class="p">])</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">nn[0]</span></tt> is the first layer. Notice that the input layer doesn&#8217;t have synaptic
weights, so they are not considered here &#8211; in other words, <tt class="docutils literal"><span class="pre">nn[0]</span></tt> is the
first hidden layer. <tt class="docutils literal"><span class="pre">nn[1]</span></tt> is the second layer which, in this case, is the
output layers. It could be accessed using <tt class="docutils literal"><span class="pre">nn[-1]</span></tt>, because a <tt class="docutils literal"><span class="pre">FeedForward</span></tt>
network is actually a list of <tt class="docutils literal"><span class="pre">Layers</span></tt>.</p>
<p>We must feed the network to get some results. Actually, we will present an
example to the network and tell it to learn the example. We create the input
vector and the desired output by the following commands:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="mf">0.9</span>
</pre></div>
</div>
<p>Let&#8217;s see what this neural network outputs with this input. We feed the neural
network and receive its output by calling it as a function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">array([[ 0.51530264]])</span>
</pre></div>
</div>
<p>We tell the network to learn the example by showing it to the <tt class="docutils literal"><span class="pre">feed()</span></tt> method,
as shown below:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="go">0.3846973641912852</span>
</pre></div>
</div>
<p>This method outputs the error obtained with the example. Let&#8217;s inspect the
synaptic weights now and see how they were modified:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="p">[</span><span class="mf">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span>
<span class="go">array([[ 0.5002258 ,  0.50005645],</span>
<span class="go">       [-0.5002258 , -0.50005645]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="p">[</span><span class="mf">1</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span>
<span class="go">array([[ 0.25299043, -0.24818621]])</span>
</pre></div>
</div>
<p>We can see that the error, for this example, is now a little smaller:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9</span> <span class="o">-</span> <span class="n">nn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">array([[ 0.38405579]])</span>
</pre></div>
</div>
<p>Notice that the answer of the neural network is a column-vector of the outputs
of the neurons in the last layer. That is why the last command resulted in an
array.</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <h4>Previous topic</h4>
            <p class="topless"><a href="tutorial.html"
                                  title="previous chapter">Tutorials</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="custom-activation.html"
                                  title="next chapter">Using Custom Activation Functions</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/tutorial/basic-neural-network.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="custom-activation.html" title="Using Custom Activation Functions"
             >next</a> |</li>
        <li class="right" >
          <a href="tutorial.html" title="Tutorials"
             >previous</a> |</li>
        <li><a href="../index.html">Peach v0.3.1 documentation</a> &raquo;</li>
          <li><a href="tutorial.html" >Tutorials</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2009, José Alexandre Nalon.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.6.2.
    </div>
  </body>
</html>