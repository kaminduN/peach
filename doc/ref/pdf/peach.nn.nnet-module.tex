%
% API Documentation for Peach - Computational Intelligence for Python
% Module peach.nn.nnet
%
% Generated by epydoc 3.0.1
% [Mon Jan 24 15:39:52 2011]
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          Module Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}|(}
\section{Module peach.nn.nnet}

    \label{peach:nn:nnet}

Basic topologies of neural networks.

This sub-package implements various neural network topologies, see the complete
list below. These topologies are implemented using the \texttt{Layer} class of the
\texttt{base} sub-package. Please, consult the documentation of that module for more
information on layers of neurons. The neural nets implemented here don't derive
from the \texttt{Layer} class, instead, they have instance variables to take control
of them. Thus, there is no base class for networks. While subclassing the
classes of this module is usually safe, it is recomended that a new kind of
net is developed from the ground up.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Functions                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Functions}

    \label{peach:nn:nnet:randn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.randn \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{randn}(\textit{d0}, \textit{d1}, \textit{dn}, \textit{...})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Returns zero-mean, unit-variance Gaussian random numbers in an
array of shape (d0, d1, ..., dn).
%
\begin{description}
\item[{Note:  This is a convenience function. If you want an}] \leavevmode 
interface that takes a tuple as the first argument
use numpy.random.standard\_normal(shape\_tuple).

\end{description}
\setlength{\parskip}{1ex}
    \end{boxedminipage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               Variables                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright \_\-\_\-d\-o\-c\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{...}}&\\
\cline{1-2}
\raggedright \_\-\_\-p\-a\-c\-k\-a\-g\-e\-\_\-\_\- & \raggedright \textbf{Value:} 
{\tt \texttt{'}\texttt{peach.nn}\texttt{'}}&\\
\cline{1-2}
\raggedright a\-r\-c\-t\-a\-n\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'arctan'{\textgreater}}&\\
\cline{1-2}
\raggedright c\-o\-s\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'cosh'{\textgreater}}&\\
\cline{1-2}
\raggedright e\-x\-p\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'exp'{\textgreater}}&\\
\cline{1-2}
\raggedright p\-i\- & \raggedright \textbf{Value:} 
{\tt 3.14159265359}&\\
\cline{1-2}
\raggedright t\-a\-n\-h\- & \raggedright \textbf{Value:} 
{\tt {\textless}ufunc 'tanh'{\textgreater}}&\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}|(}
\subsection{Class FeedForward}

    \label{peach:nn:nnet:FeedForward}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for list, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{list}\multirow{2}{\BCL}{list}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.nn.nnet.FeedForward}}
\end{tabular}


Classic completely connected neural network.

A feedforward neural network is implemented as a list of layers, each layer
being a \texttt{Layer} object (please consult the documentation on the \texttt{base}
module for more information on layers). The layers are completely connected,
which means that every neuron in one layers is connected to every other
neuron in the following layer.

There is a number of learning methods that are already implemented, but in
general, any learning class derived from \texttt{FFLearning} can be used. No
other kind of learning can be used. Please, consult the documentation on the
\texttt{lrules} (\emph{learning rules}) module.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{layers}, \textit{phi}={\tt {\textless}class 'peach.nn.af.Linear'{\textgreater}}, \textit{lrule}={\tt {\textless}class 'peach.nn.lrules.BackPropagation'{\textgreater}}, \textit{bias}={\tt False})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes a feedforward neural network.

A feedforward network is implemented as a list of layers, completely
connected.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxx}

          \item[layers]


A list of integers containing the shape of the network. The first
element of the list is the number of inputs of the network (or, as
somebody prefer, the number of input neurons); the number of outputs
is the number of neurons in the last layer. Thus, at least two
numbers should be given.
          \item[phi]


The activation functions to be used with each layer of the network.
Please consult the \texttt{Layer} documentation in the \texttt{base} module
for more information. This parameter can be a single function or a
list of functions. If only one function is given, then the same
function is used in every layer. If a list of functions is given,
then the layers use the functions in the sequence given. Note that
heterogeneous networks can be created that way. Defaults to
\texttt{Linear}.
          \item[lrule]


The learning rule used. Only \texttt{FFLearning} objects (instances of
the class or of the subclasses) are allowed. Defaults to
\texttt{BackPropagation}. Check the \texttt{lrules} documentation for more
information.
          \item[bias]


If \texttt{True}, then the neurons are biased.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

new empty list
      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \label{peach:nn:nnet:FeedForward:__call__}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}!peach.nn.nnet.FeedForward.\_\_call\_\_ \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

The feedforward method of the network.

The \texttt{\_\_call\_\_} interface should be called if the answer of the neuron
network to a given input vector \texttt{x} is desired. \emph{This method has
collateral effects}, so beware. After the calling of this method, the
\texttt{y} property is set with the activation potential and the answer of
the neurons, respectivelly.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


The input vector to the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The vector containing the answer of every neuron in the last layer, in
the respective order.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:nnet:FeedForward:learn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}!peach.nn.nnet.FeedForward.learn \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{learn}(\textit{self}, \textit{x}, \textit{d})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Applies one example of the training set to the network.

Using this method, one iteration of the learning procedure is made with
the neurons of this network. This method presents one example (not
necessarilly of a training set) and applies the learning rule over the
network. The learning rule is defined in the initialization of the
network, and some are implemented on the \texttt{lrules} method. New methods
can be created, consult the \texttt{lrules} documentation but, for
\texttt{FeedForward} instances, only \texttt{FFLearning} learning is allowed.

Also, notice that \emph{this method only applies the learning method!} The
network should be fed with the same input vector before trying to learn
anything first. Consult the \texttt{feed} and \texttt{train} methods below for
more ways to train a network.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
          \item[d]


The desired answer of the network for this particular input vector.
Notice that the desired answer should have the same dimension of the
last layer of the network. This means that a desired answer should
be given for every output of the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:nnet:FeedForward:feed}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}!peach.nn.nnet.FeedForward.feed \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{feed}(\textit{self}, \textit{x}, \textit{d})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Feed the network and applies one example of the training set to the
network.

Using this method, one iteration of the learning procedure is made with
the neurons of this network. This method presents one example (not
necessarilly of a training set) and applies the learning rule over the
network. The learning rule is defined in the initialization of the
network, and some are implemented on the \texttt{lrules} method. New methods
can be created, consult the \texttt{lrules} documentation but, for
\texttt{FeedForward} instances, only \texttt{FFLearning} learning is allowed.

Also, notice that \emph{this method feeds the network} before applying the
learning rule. Feeding the network has collateral effects, and some
properties change when this happens. Namely, the \texttt{y} property is set.
Please consult the \texttt{\_\_call\_\_} interface.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
          \item[d]


The desired answer of the network for this particular input vector.
Notice that the desired answer should have the same dimension of the
last layer of the network. This means that a desired answer should
be given for every output of the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:nnet:FeedForward:train}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}!peach.nn.nnet.FeedForward.train \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{train}(\textit{self}, \textit{train\_set}, \textit{imax}={\tt 2000}, \textit{emax}={\tt 1e-05}, \textit{randomize}={\tt False})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Presents a training set to the network.

This method automatizes the training of the network. Given a training
set, the examples are shown to the network (possibly in a randomized
way). A maximum number of iterations or a maximum admitted error should
be given as a stop condition.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxxxxx}

          \item[train\_set]


The training set is a list of examples. It can have any size and can
contain repeated examples. In fact, the definition of the training
set is open. Each element of the training set, however, should be a
two-tuple \texttt{(x, d)}, where \texttt{x} is the input vector, and \texttt{d} is
the desired response of the network for this particular input. See
the \texttt{learn} and \texttt{feed} for more information.
          \item[imax]


The maximum number of iterations. Examples from the training set
will be presented to the network while this limit is not reached.
Defaults to 2000.
          \item[emax]


The maximum admitted error. Examples from the training set will be
presented to the network until the error obtained is lower than this
limit. Defaults to 1e-5.
          \item[randomize]


If this is \texttt{True}, then the examples are shown in a randomized
order. If \texttt{False}, then the examples are shown in the same order
that they appear in the \texttt{train\_set} list. Defaults to \texttt{False}.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from list}}}

\begin{quote}
\_\_add\_\_(), \_\_contains\_\_(), \_\_delitem\_\_(), \_\_delslice\_\_(), \_\_eq\_\_(), \_\_ge\_\_(), \_\_getattribute\_\_(), \_\_getitem\_\_(), \_\_getslice\_\_(), \_\_gt\_\_(), \_\_iadd\_\_(), \_\_imul\_\_(), \_\_iter\_\_(), \_\_le\_\_(), \_\_len\_\_(), \_\_lt\_\_(), \_\_mul\_\_(), \_\_ne\_\_(), \_\_new\_\_(), \_\_repr\_\_(), \_\_reversed\_\_(), \_\_rmul\_\_(), \_\_setitem\_\_(), \_\_setslice\_\_(), \_\_sizeof\_\_(), append(), count(), extend(), index(), insert(), pop(), remove(), reverse(), sort()
\end{quote}

\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_setattr\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright n\-l\-a\-y\-e\-r\-s\- & &\\
\cline{1-2}
\raggedright b\-i\-a\-s\- & &\\
\cline{1-2}
\raggedright y\- & &\\
\cline{1-2}
\raggedright p\-h\-i\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                            Class Variables                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Class Variables}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\multicolumn{2}{|l|}{\textit{Inherited from list}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_hash\_\_}\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.FeedForward \textit{(class)}|)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Class Description                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.SOM \textit{(class)}|(}
\subsection{Class SOM}

    \label{peach:nn:nnet:SOM}
\begin{tabular}{cccccccc}
% Line for object, linespec=[False, False]
\multicolumn{2}{r}{\settowidth{\BCL}{object}\multirow{2}{\BCL}{object}}
&&
&&
  \\\cline{3-3}
  &&\multicolumn{1}{c|}{}
&&
&&
  \\
% Line for peach.nn.base.Layer, linespec=[False]
\multicolumn{4}{r}{\settowidth{\BCL}{peach.nn.base.Layer}\multirow{2}{\BCL}{peach.nn.base.Layer}}
&&
  \\\cline{5-5}
  &&&&\multicolumn{1}{c|}{}
&&
  \\
&&&&\multicolumn{2}{l}{\textbf{peach.nn.nnet.SOM}}
\end{tabular}


A Self-Organizing Map (SOM).

A self-organizing map is a type of neural network that is trained via
unsupervised learning. In particular, the self-organizing map finds the
neuron closest to an input vector -{}- this neuron is the winning neuron, and
it is the answer of the network. Thus, the SOM is usually used for
classification and pattern recognition.

The SOM is a single-layer network, so this class subclasses the \texttt{Layer}
class. But some of the properties of a \texttt{Layer} object are not available or
make no sense in this context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                Methods                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Methods}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_init\_\_}(\textit{self}, \textit{shape}, \textit{lrule}={\tt {\textless}class 'peach.nn.lrules.Competitive'{\textgreater}})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Initializes a self-organizing map.

A self-organizing map is implemented as a layer of neurons. There is no
connection among the neurons. The answer to a given input is the neuron
closer to the given input. \texttt{phi} (the activation function) \texttt{v} (the
activation potential) and \texttt{bias} are not used.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[shape]


Stablishes the size of the SOM. It must be a two-tuple of the
format \texttt{(m, n)}, where \texttt{m} is the number of neurons in the
layer, and \texttt{n} is the number of inputs of each neuron. The neurons
in the layer all have the same number of inputs.
          \item[lrule]


The learning rule used. Only \texttt{SOMLearning} objects (instances of
the class or of the subclasses) are allowed. Defaults to
\texttt{Competitive}. Check the \texttt{lrules} documentation for more
information.
        \end{Ventry}

      \end{quote}

      Overrides: object.\_\_init\_\_

    \end{boxedminipage}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{\_\_call\_\_}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

The response of the network to a given input.

The \texttt{\_\_call\_\_} interface should be called if the answer of the neuron
network to a given input vector \texttt{x} is desired. \emph{This method has
collateral effects}, so beware. After the calling of this method, the
\texttt{y} property is set with the activation potential and the answer of
the neurons, respectivelly.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


The input vector to the network.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The winning neuron.
      \end{quote}

      Overrides: peach.nn.base.Layer.\_\_call\_\_

    \end{boxedminipage}

    \label{peach:nn:nnet:SOM:learn}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.SOM \textit{(class)}!peach.nn.nnet.SOM.learn \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{learn}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Applies one example of the training set to the network.

Using this method, one iteration of the learning procedure is made with
the neurons of this network. This method presents one example (not
necessarilly of a training set) and applies the learning rule over the
network. The learning rule is defined in the initialization of the
network, and some are implemented on the \texttt{lrules} method. New methods
can be created, consult the \texttt{lrules} documentation but, for
\texttt{SOM} instances, only \texttt{SOMLearning} learning is allowed.

Also, notice that \emph{this method only applies the learning method!} The
network should be fed with the same input vector before trying to learn
anything first. Consult the \texttt{feed} and \texttt{train} methods below for
more ways to train a network.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:nnet:SOM:feed}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.SOM \textit{(class)}!peach.nn.nnet.SOM.feed \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{feed}(\textit{self}, \textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Feed the network and applies one example of the training set to the
network.

Using this method, one iteration of the learning procedure is made with
the neurons of this network. This method presents one example (not
necessarilly of a training set) and applies the learning rule over the
network. The learning rule is defined in the initialization of the
network, and some are implemented on the \texttt{lrules} method. New methods
can be created, consult the \texttt{lrules} documentation but, for
\texttt{SOM} instances, only \texttt{SOMLearning} learning is allowed.

Also, notice that \emph{this method feeds the network} before applying the
learning rule. Feeding the network has collateral effects, and some
properties change when this happens. Namely, the \texttt{y} property is set.
Please consult the \texttt{\_\_call\_\_} interface.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Input vector of the example. It should be a column vector of the
correct dimension, that is, the number of input neurons.
        \end{Ventry}

      \end{quote}

      \textbf{Return Value}
    \vspace{-1ex}

      \begin{quote}

The error obtained by the network.
      \end{quote}

    \end{boxedminipage}

    \label{peach:nn:nnet:SOM:train}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.SOM \textit{(class)}!peach.nn.nnet.SOM.train \textit{(method)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{train}(\textit{self}, \textit{train\_set}, \textit{imax}={\tt 2000}, \textit{emax}={\tt 1e-05}, \textit{randomize}={\tt False})

    \vspace{-1.5ex}

    \rule{\textwidth}{0.5\fboxrule}
\setlength{\parskip}{2ex}

Presents a training set to the network.

This method automatizes the training of the network. Given a training
set, the examples are shown to the network (possibly in a randomized
way). A maximum number of iterations or a maximum admitted error should
be given as a stop condition.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxxxxxx}

          \item[train\_set]


The training set is a list of examples. It can have any size and can
contain repeated examples. In fact, the definition of the training
set is open. Each element of the training set, however, should be a
input vector of the correct dimensions, See the \texttt{learn} and
\texttt{feed} for more information.
          \item[imax]


The maximum number of iterations. Examples from the training set
will be presented to the network while this limit is not reached.
Defaults to 2000.
          \item[emax]


The maximum admitted error. Examples from the training set will be
presented to the network until the error obtained is lower than this
limit. Defaults to 1e-5.
          \item[randomize]


If this is \texttt{True}, then the examples are shown in a randomized
order. If \texttt{False}, then the examples are shown in the same order
that they appear in the \texttt{train\_set} list. Defaults to \texttt{False}.
        \end{Ventry}

      \end{quote}

    \end{boxedminipage}


\large{\textbf{\textit{Inherited from peach.nn.base.Layer\textit{(Section \ref{peach:nn:base:Layer})}}}}

\begin{quote}
\_\_getitem\_\_(), \_\_setitem\_\_()
\end{quote}

\large{\textbf{\textit{Inherited from object}}}

\begin{quote}
\_\_delattr\_\_(), \_\_format\_\_(), \_\_getattribute\_\_(), \_\_hash\_\_(), \_\_new\_\_(), \_\_reduce\_\_(), \_\_reduce\_ex\_\_(), \_\_repr\_\_(), \_\_setattr\_\_(), \_\_sizeof\_\_(), \_\_str\_\_(), \_\_subclasshook\_\_()
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Properties                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsubsection{Properties}

    \vspace{-1cm}
\hspace{\varindent}\begin{longtable}{|p{\varnamewidth}|p{\vardescrwidth}|l}
\cline{1-2}
\cline{1-2} \centering \textbf{Name} & \centering \textbf{Description}& \\
\cline{1-2}
\endhead\cline{1-2}\multicolumn{3}{r}{\small\textit{continued on next page}}\\\endfoot\cline{1-2}
\endlastfoot\raggedright y\- & &\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from peach.nn.base.Layer \textit{(Section \ref{peach:nn:base:Layer})}}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright bias, inputs, phi, shape, size, v, weights}\\
\cline{1-2}
\multicolumn{2}{|l|}{\textit{Inherited from object}}\\
\multicolumn{2}{|p{\varwidth}|}{\raggedright \_\_class\_\_}\\
\cline{1-2}
\end{longtable}

    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}!peach.nn.nnet.SOM \textit{(class)}|)}
    \index{peach \textit{(package)}!peach.nn \textit{(package)}!peach.nn.nnet \textit{(module)}|)}
